install.packages("tidyverse")
install.packages("caret")
install.packages("stringi")
library(tidyverse)  #tidyverse for easy data manipulation and visualization
library(caret)      #caret for easy machine learning workflow
library(rpart)     #rpart for computing decision tree models
data(iris)
iris
library(rpart)
model <- rpart(Species ~., data = iris)
par(xpd = NA) # otherwise on some devices the text is clipped
plot(model)
text(model, digits = 3)
print(model, digits = 2)
library(numDeriv)
library(dplyr)
library(magrittr)
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
sample_n(PimaIndiansDiabetes2, 3)
set.seed(12345)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
set.seed(123)
model1 <- rpart(diabetes ~., data = train.data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(test.data, type = "class")
head(predicted.classes)
plot(predicted.classes)
mean(predicted.classes == test.data$diabetes)
#The overall accuracy of our tree model is 78%, which is not so bad.
#this full tree including all predictor appears to be very complex and can be difficult to interpret in the situation where you have a large data sets with multiple predictors.
#Additionally, it is easy to see that, a fully grown tree will overfit the training data and might lead to poor test set performance.
#A strategy to limit this overfitting is to prune back the tree resulting to a simpler tree with fewer splits and better interpretation at the cost of a little bias (James et al. 2014, P. Bruce and Bruce (2017)).
#---------------------------------Pruning the tree------------------------
#Briefly, our goal here is to see if a smaller subtree can give us comparable results to the fully grown tree. If yes, we should go for the simpler tree because it reduces the likelihood of overfitting.
#One possible robust strategy of pruning the tree (or stopping the tree to grow) consists of avoiding splitting a partition if the split does not significantly improves the overall quality of the model.
#In rpart package, this is controlled by the complexity parameter (cp), which imposes a penalty to the tree for having two many splits. The default value is 0.01. The higher the cp, the smaller the tree.
#A too small value of cp leads to overfitting and a too large cp value will result to a too small tree. Both cases decrease the predictive performance of the model.
#An optimal cp value can be estimated by testing different cp values and using cross-validation approaches to determine the corresponding prediction accuracy of the model. The best cp is then defined as the one that maximize the cross-validation accuracy (Chapter @ref(cross-validation)).
#Pruning can be easily performed in the caret package workflow, which invokes the rpart method for automatically testing different possible values of cp, then choose the optimal cp that maximize the cross-validation ("cv") accuracy, and fit the final best CART model that explains the best our data.
#You can use the following arguments in the function train() [from caret package]:
#trControl, to set up 10-fold cross validation
set.seed(123)
model2 <- train(
diabetes ~., data = train.data, method = "rpart",
trControl = trainControl("cv", number = 10),
tuneLength = 10)
# Plot model accuracy vs different values of cp (complexity parameter)
plot(model2)
# Print the best tuning parameter cp that maximizes the model accuracy
model2$bestTune
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model2$finalModel)
text(model2$finalModel,  digits = 3)
model2$finalModel
# Make predictions on the test data
predicted.classes <- model2 %>% predict(test.data)
# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)
#From the output above, it can be seen that the best value for the complexity parameter (cp) is 0.032, allowing a simpler tree, easy to interpret, with an overall accuracy of 79%, which is comparable to the accuracy (78%) that we have obtained with the full tree. The prediction accuracy of the pruned tree is even better compared to the full tree. so we should go for simplest model.
#============================regression tree===========================
data("Boston", package = "MASS")
# Inspect the data
sample_n(Boston, 3)
set.seed(123)
training.samples <- Boston$medv %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
#Here, the best cp value is the one that minimize the prediction error RMSE (root mean squared error).
set.seed(123)
model <- train(
medv ~., data = train.data, method = "rpart",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
plot(model)
model$bestTune
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model$finalModel)
text(model$finalModel, digits = 3)
# Decision rules in the model
model$finalModel
predictions <- model %>% predict(test.data)
head(predictions)
# Compute the prediction error RMSE
RMSE(predictions, test.data$medv)
library(tidyverse)  #tidyverse for easy data manipulation and visualization
library(caret)      #caret for easy machine learning workflow
library(randomForest)  #randomForest for computing random forest algorithm
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
set.seed(123)
model <- train(
diabetes ~., data = train.data, method = "rf",
trControl = trainControl("cv", number = 10),
importance = TRUE
)
# Best tuning parameter
model$bestTune
model$finalModel
# Make predictions on the test data
predicted.classes <- model %>% predict(test.data)
plot(model$finalModel)
head(predicted.classes)
# Compute model accuracy rate
mean(predicted.classes == test.data$diabetes)
#By default, 500 trees are trained. The optimal number of variables sampled at each split is 5.
#Each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations (James et al. 2014).
#For a given tree, the out-of-bag (OOB) error is the model error in predicting the data left out of the training set for that tree (P. Bruce and Bruce 2017). OOB is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach.
library(randomForest)
importance(model$finalModel)
#MeanDecreaseAccuracy, which is the average decrease of model accuracy in predicting the outcome of the out-of-bag samples when a specific variable is excluded from the model.
#MeanDecreaseGini, which is the average decrease in node impurity that results from splits over that variable. The Gini impurity index is only used for classification problem.
#Note:: that, by default (argument importance = FALSE), randomForest only calculates the Gini impurity index. However, computing the model accuracy by variable (argument importance = TRUE) requires supplementary computations which might be time consuming in the situations, where thousands of models (trees) are being fitted.
# Plot MeanDecreaseAccuracy
varImpPlot(model$finalModel, type = 1)   #randomForest package
# Plot MeanDecreaseGini
varImpPlot(model$finalModel, type = 2)
#The results show that across all of the trees considered in the random forest, the glucose and age variables are the two most important variables.
#The function varImp() [in caret] displays the importance of variables in percentage:
varImp(model)    #package caret
data("Boston", package = "MASS")
# Inspect the data
sample_n(Boston, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
#-----Computing random forest regression trees
#Here the prediction error is measured by the RMSE,
#RMSE is computed as RMSE = mean((observeds - predicteds)^2) %>% sqrt().
#The lower the RMSE, the better the model.
# Fit the model on the training set
set.seed(123)
model <- train(
medv ~., data = train.data, method = "rf",
trControl = trainControl("cv", number = 10)
)
# Best tuning parameter mtry
model$bestTune
# Make predictions on the test data
predictions <- model %>% predict(test.data)
plot(predictions)
head(predictions)
# Compute the average prediction error RMSE
RMSE(predictions, test.data$medv)
